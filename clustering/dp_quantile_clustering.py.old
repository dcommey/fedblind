# clustering/dp_quantile_clustering.py

import numpy as np
from scipy.stats import laplace
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from .base_clustering import DPBaseClustering

class DPQuantileClustering(DPBaseClustering):
    def __init__(self, max_clusters=10, epsilon=1.0, num_quantiles=5, dataset_name='mnist', cluster_on='features'):
        super().__init__(max_clusters, epsilon, cluster_on)
        self.num_quantiles = num_quantiles
        self.dataset_name = dataset_name
        self.num_classes = self.get_num_classes(dataset_name)

    def get_num_classes(self, dataset_name):
        dataset_classes = {
            'mnist': 10,
            'cifar10': 10,
            'cifar100': 100,
            'svhn': 10,
            'har': 6  # Assuming 6 activities for HAR dataset
        }
        return dataset_classes.get(dataset_name, 10)  # Default to 10 if dataset is unknown

    def cluster_clients(self, client_dataloaders):
        client_stats = self.compute_client_statistics(client_dataloaders)
        noisy_stats = self.apply_differential_privacy(client_stats)
        
        if self.num_clusters is None:
            self.num_clusters = self.find_optimal_clusters(noisy_stats)
        
        self.model = KMeans(n_clusters=self.num_clusters, random_state=42)
        cluster_assignments = self.model.fit_predict(noisy_stats)
        
        quality_metrics = self.compute_clustering_quality(noisy_stats, cluster_assignments)
        
        return cluster_assignments, quality_metrics

    def compute_client_statistics(self, client_dataloaders):
        if self.cluster_on == 'features':
            return self.compute_feature_quantiles(client_dataloaders)
        elif self.cluster_on == 'labels':
            return self.compute_label_quantiles(client_dataloaders)
        else:
            raise ValueError("cluster_on must be either 'features' or 'labels'")

    def compute_feature_quantiles(self, client_dataloaders):
        client_stats = []
        for dataloader in client_dataloaders:
            all_data = []
            for batch, _ in dataloader:
                all_data.append(batch.numpy())
            all_data = np.concatenate(all_data, axis=0)
            
            # Flatten the data if it's multi-dimensional
            flattened_data = all_data.reshape(all_data.shape[0], -1)
            
            # Compute quantiles for each feature
            quantile_features = []
            for feature in range(flattened_data.shape[1]):
                quantiles = np.quantile(flattened_data[:, feature], 
                                        np.linspace(0, 1, self.num_quantiles + 2)[1:-1])
                quantile_features.extend(quantiles)
            
            client_stats.append(quantile_features)
        return np.array(client_stats)

    def compute_label_quantiles(self, client_dataloaders):
        client_stats = []
        for dataloader in client_dataloaders:
            all_labels = []
            for _, labels in dataloader:
                all_labels.extend(labels.numpy())
            
            # Compute label distribution
            label_dist = np.bincount(all_labels, minlength=self.num_classes) / len(all_labels)
            
            # Compute quantiles of the label distribution
            quantiles = np.quantile(label_dist, np.linspace(0, 1, self.num_quantiles + 2)[1:-1])
            
            client_stats.append(quantiles)
        return np.array(client_stats)

    def apply_differential_privacy(self, client_stats):
        sensitivity = 1.0 / len(client_stats)  # Assuming quantiles are in [0, 1]
        noise_scale = sensitivity / self.epsilon
        noisy_stats = client_stats + laplace.rvs(loc=0, scale=noise_scale, size=client_stats.shape)
        return np.clip(noisy_stats, 0, 1)  # Ensure valid range

    def find_optimal_clusters(self, data):
        n_samples, n_features = data.shape
        max_clusters = min(self.max_clusters, n_samples - 1)
        
        if max_clusters < 2:
            return 1  # Not enough samples for meaningful clustering
        
        # If the data is high-dimensional, apply PCA
        if n_features > 50:  # You can adjust this threshold
            pca = PCA(n_components=min(50, n_samples - 1))
            data = pca.fit_transform(data)
        
        silhouette_scores = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(data)
            
            if k < n_samples:
                silhouette_scores.append(silhouette_score(data, labels))
            else:
                silhouette_scores.append(-1)  # Invalid score

        return np.argmax(silhouette_scores) + 2  # +2 because we started from k=2

    def compute_clustering_quality(self, data, labels):
        if len(np.unique(labels)) < 2:
            return {
                "silhouette_score": None,
                "calinski_harabasz_score": None,
                "davies_bouldin_score": None
            }
        
        return {
            "silhouette_score": silhouette_score(data, labels),
            "calinski_harabasz_score": calinski_harabasz_score(data, labels),
            "davies_bouldin_score": davies_bouldin_score(data, labels)
        }